"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[71710],{3905:(e,n,t)=>{t.d(n,{Zo:()=>d,kt:()=>g});var a=t(67294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var l=a.createContext({}),p=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},d=function(e){var n=p(e.components);return a.createElement(l.Provider,{value:n},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),c=p(t),m=i,g=c["".concat(l,".").concat(m)]||c[m]||u[m]||o;return t?a.createElement(g,r(r({ref:n},d),{},{components:t})):a.createElement(g,r({ref:n},d))}));function g(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var o=t.length,r=new Array(o);r[0]=m;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[c]="string"==typeof e?e:i,r[1]=s;for(var p=2;p<o;p++)r[p]=t[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},44596:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>c,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var a=t(87462),i=(t(67294),t(3905));const o={},r="question-answering-using-gpt3-examples",s={unversionedId:"OpenAI/Question-Answering/question-answering-using-gpt3-examples",id:"OpenAI/Question-Answering/question-answering-using-gpt3-examples",title:"question-answering-using-gpt3-examples",description:"- https://www.pragnakalp.com/question-answering-using-gpt3-examples/",source:"@site/docs/OpenAI/Question-Answering/question-answering-using-gpt3-examples.md",sourceDirName:"OpenAI/Question-Answering",slug:"/OpenAI/Question-Answering/question-answering-using-gpt3-examples",permalink:"/notebook/docs/OpenAI/Question-Answering/question-answering-using-gpt3-examples",draft:!1,editUrl:"https://github.com/scheffershen/notebook/tree/main/docs/OpenAI/Question-Answering/question-answering-using-gpt3-examples.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"How to build an AI that can answer questions about your website",permalink:"/notebook/docs/OpenAI/Question-Answering/How to build an AI that can answer questions about your website"},next:{title:"awesome-chatgpt-prompts",permalink:"/notebook/docs/OpenAI/awesome-chatgpt-prompts"}},l={},p=[{value:"OpenAI",id:"openai",level:2},{value:"GPT-3",id:"gpt-3",level:2},{value:"Signup at OpenAI and get an API Key",id:"signup-at-openai-and-get-an-api-key",level:2},{value:"How to use GPT-3 Question Answering",id:"how-to-use-gpt-3-question-answering",level:2},{value:"More examples",id:"more-examples",level:3},{value:"Building a Knowledge Base",id:"building-a-knowledge-base",level:3},{value:"Data Preparation",id:"data-preparation",level:3}],d={toc:p};function c(e){let{components:n,...t}=e;return(0,i.kt)("wrapper",(0,a.Z)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"question-answering-using-gpt3-examples"},"question-answering-using-gpt3-examples"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://www.pragnakalp.com/question-answering-using-gpt3-examples/"},"https://www.pragnakalp.com/question-answering-using-gpt3-examples/")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://www.pinecone.io/learn/openai-gen-qa/"},"https://www.pinecone.io/learn/openai-gen-qa/"))),(0,i.kt)("p",null,"For any existing knowledgebase for your product or service, it is easier than ever to create an AI powered add-on that can search the knowledgebase and answer questions in natural language."),(0,i.kt)("p",null,"Using GPT-3 (developed by OpenAI), one can easily build an API driven service with great results, without actually learning or implementing any Machine Learning code or putting heavy compute power to support it."),(0,i.kt)("h2",{id:"openai"},"OpenAI"),(0,i.kt)("p",null,"OpenAI is an artificial intelligence (AI) research laboratory consisting of the for-profit corporation OpenAI LP and its parent company, the non-profit OpenAI Inc. \u2014 Wikipedia"),(0,i.kt)("h2",{id:"gpt-3"},"GPT-3"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"Generative Pre-trained Transformer 3")," (GPT-3) is an autoregressive language model that uses deep learning to produce human-like text. \u2014 Wikipedia."),(0,i.kt)("p",null,"GPT-3 is the third-generation language prediction model developed by OpenAI. "),(0,i.kt)("p",null,"It is said that the quality of text generated by GPT-3 is so high, that it can be very difficult to determine if it was generated by a human or AI. "),(0,i.kt)("p",null,"Now, let\u2019s see how we can use GPT-3 to create an answering service on top of our existing knowledgebase."),(0,i.kt)("h2",{id:"signup-at-openai-and-get-an-api-key"},"Signup at OpenAI and get an API Key"),(0,i.kt)("h2",{id:"how-to-use-gpt-3-question-answering"},"How to use GPT-3 Question Answering"),(0,i.kt)("p",null,"Create & Activate virtualenv"),(0,i.kt)("p",null,"   $ py -3.9 -m "),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Let\u2019s explore how we can perform question answering using a document list ")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import openai\nopenai.api_key = "YOUR-API-KEY"\n \ndocument_list = ["Google was founded in 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University in California. Together they own about 14 percent of its shares and control 56 percent of the stockholder voting power through supervoting stock. They incorporated Google as a privately held company on September 4, 1998. An initial public offering (IPO) took place on August 19, 2004, and Google moved to its headquarters in Mountain View, California, nicknamed the Googleplex. In August 2015, Google announced plans to reorganize its various interests as a conglomerate called Alphabet Inc. Google is Alphabet\'s leading subsidiary and will continue to be the umbrella company for Alphabet\'s Internet interests. Sundar Pichai was appointed CEO of Google, replacing Larry Page who became the CEO of Alphabet.",\n"Amazon is an American multinational technology company based in Seattle, Washington, which focuses on e-commerce, cloud computing, digital streaming, and artificial intelligence. It is one of the Big Five companies in the U.S. information technology industry, along with Google, Apple, Microsoft, and Facebook. The company has been referred to as \'one of the most influential economic and cultural forces in the world\', as well as the world\'s most valuable brand. Jeff Bezos founded Amazon from his garage in Bellevue, Washington on July 5, 1994. It started as an online marketplace for books but expanded to sell electronics, software, video games, apparel, furniture, food, toys, and jewelry. In 2015, Amazon surpassed Walmart as the most valuable retailer in the United States by market capitalization."]\n \nresponse = openai.Answer.create(\n search_model="ada",\n model="curie",\n question="when was google founded?",\n documents=document_list,\n examples_context="In 2017, U.S. life expectancy was 78.6 years.",\n examples=[["What is human life expectancy in the United States?","78 years."]],\n max_tokens=10,\n stop=["\\n", "<|endoftext|>"],\n)\n \nprint(response)\n\n')),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},"Let\u2019s see how we can perform Question Answering using data stored in File :")),(0,i.kt)("p",null,"You can create a .jsonl(JSON Lines) files where each JSON line contains the ",(0,i.kt)("inlineCode",{parentName:"p"},"text")," field and ",(0,i.kt)("inlineCode",{parentName:"p"},"metadata")," field(optional). "),(0,i.kt)("p",null,'   {"text": "Hello OpenAI", "metadata": "sample data"}'),(0,i.kt)("p",null,"Once you have created the jsonl file, now you need to upload that file."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import openai\nopenai.api_key = "YOUR-API-KEY"\n \nresponse = openai.File.create(\n file=open("sample_qna.jsonl"),\n purpose=\'answers\'\n)\n \nprint(response)\n')),(0,i.kt)("p",null,"From the above response, we will need the file ID"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import openai\nopenai.api_key = "YOUR-API-KEY"\n \nresponse = openai.Answer.create(\n   search_model="ada",\n   model="curie",\n   question="when was google founded?",\n   file="file-h5zzNVGdUigntPQWeVmJqAJf",\n   examples_context="In 2017, U.S. life expectancy was 78.6 years.",\n   examples=[["What is human life expectancy in the United States?","78 years."]],\n   max_rerank=10,\n   max_tokens=10,\n   stop=["\\n", "<|endoftext|>"]\n \n)\n \nprint(response)\n')),(0,i.kt)("p",null,"NOTE : you can specify either file or document not both"),(0,i.kt)("h3",{id:"more-examples"},"More examples"),(0,i.kt)("p",null,"example 1:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import openai\nopenai.api_key = \"YOUR-API-KEY\"\n\nquery = \"who was the 12th person on the moon and when did they land?\"\n\n# now query text-davinci-003 WITHOUT context\nres = openai.Completion.create(\n    engine='text-davinci-003',\n    prompt=query,\n    temperature=0,\n    max_tokens=400,\n    top_p=1,\n    frequency_penalty=0,\n    presence_penalty=0,\n    stop=None\n)\n\nres['choices'][0]['text'].strip()\n")),(0,i.kt)("p",null,"example 2:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import openai\nopenai.api_key = \"YOUR-API-KEY\"\n\n# first let's make it simpler to get answers\ndef complete(prompt):\n    # query text-davinci-003\n    res = openai.Completion.create(\n        engine='text-davinci-003',\n        prompt=prompt,\n        temperature=0,\n        max_tokens=400,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n        stop=None\n    )\n    return res['choices'][0]['text'].strip()\n\nquery = (\n    \"Which training method should I use for sentence transformers when \" +\n    \"I only have pairs of related sentences?\"\n)\n\ncomplete(query)\n")),(0,i.kt)("h3",{id:"building-a-knowledge-base"},"Building a Knowledge Base"),(0,i.kt)("p",null,"We refer to knowledge bases that can enable the retrieval of semantically relevant information as vector databases."),(0,i.kt)("p",null,"A vector database stores vector representations of information encoded using specific ML models."),(0,i.kt)("p",null,"These models have an \u201cunderstanding\u201d of language and can encode passages with similar meanings into a similar vector space and dissimilar passages into a dissimilar vector space."),(0,i.kt)("p",null,"Weaviate is an open-source vector search engine."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"embed_model = \"text-embedding-ada-002\"\n\nres = openai.Embedding.create(\n    input=[\n        \"Sample document text goes here\",\n        \"there will be several phrases in each batch\"\n    ], engine=embed_model\n)\n\n# vector embeddings are stored within the 'data' key\nres.keys()\n\ndict_keys(['object', 'data', 'model', 'usage'])\n\n# we have created two 1536-dimensional vectors\nlen(res['data'][0]['embedding']), len(res['data'][1]['embedding'])\n")),(0,i.kt)("h3",{id:"data-preparation"},"Data Preparation"),(0,i.kt)("p",null,"The dataset we will use in our knowledge base is the jamescalam/youtube-transcriptions dataset hosted on Hugging Face Datasets"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from datasets import load_dataset\n\ndata = load_dataset('jamescalam/youtube-transcriptions', split='train')\ndata\n\ndata[0]\n\n#The dataset contains many small snippets of text data. We need to merge several snippets to create more substantial chunks of text that contain more meaningful information.\nfrom tqdm.auto import tqdm\n\nnew_data = []\n\nwindow = 20  # number of sentences to combine\nstride = 4  # number of sentences to 'stride' over, used to create overlap\n\nfor i in tqdm(range(0, len(data), stride)):\n    i_end = min(len(data)-1, i+window)\n    if data[i]['title'] != data[i_end]['title']:\n        # in this case we skip this entry as we have start/end of two videos\n        continue\n    text = ' '.join(data[i:i_end]['text'])\n    # create the new merged dataset\n    new_data.append({\n        'start': data[i]['start'],\n        'end': data[i_end]['end'],\n        'title': data[i]['title'],\n        'text': text,\n        'id': data[i]['id'],\n        'url': data[i]['url'],\n        'published': data[i]['published'],\n        'channel_id': data[i]['channel_id']\n    })\n\nnew_data[0]\n\n#Creating the Vector Database\n#The vector database is the storage and retrieval component in our pipeline.\nimport pinecone\n\nindex_name = 'openai-youtube-transcriptions'\n\n# initialize connection (get API key at app.pinecone.io)\npinecone.init(\n    api_key=\"YOUR_API_KEY\",\n    environment=\"YOUR_ENV\"  # find next to API key\n)\n\n# check if index already exists (it shouldn't if this is first time)\nif index_name not in pinecone.list_indexes():\n    # if does not exist, create index\n    pinecone.create_index(\n        index_name,\n        dimension=len(res['data'][0]['embedding']),\n        metric='cosine',\n        metadata_config={\n            'indexed': ['channel_id', 'published']\n        }\n    )\n# connect to index\nindex = pinecone.Index(index_name)\n# view index stats\nindex.describe_index_stats()\n\n#Then we embed and index a dataset like so:\nfrom tqdm.auto import tqdm\nimport datetime\nfrom time import sleep\n\nbatch_size = 100  # how many embeddings we create and insert at once\n\nfor i in tqdm(range(0, len(new_data), batch_size)):\n    # find end of batch\n    i_end = min(len(new_data), i+batch_size)\n    meta_batch = new_data[i:i_end]\n    # get ids\n    ids_batch = [x['id'] for x in meta_batch]\n    # get texts to encode\n    texts = [x['text'] for x in meta_batch]\n    # create embeddings (try-except added to avoid RateLimitError)\n    try:\n        res = openai.Embedding.create(input=texts, engine=embed_model)\n    except:\n        done = False\n        while not done:\n            sleep(5)\n            try:\n                res = openai.Embedding.create(input=texts, engine=embed_model)\n                done = True\n            except:\n                pass\n    embeds = [record['embedding'] for record in res['data']]\n    # cleanup metadata\n    meta_batch = [{\n        'start': x['start'],\n        'end': x['end'],\n        'title': x['title'],\n        'text': x['text'],\n        'url': x['url'],\n        'published': x['published'],\n        'channel_id': x['channel_id']\n    } for x in meta_batch]\n    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n    # upsert to Pinecone\n    index.upsert(vectors=to_upsert)\n\n#We\u2019re ready to combine OpenAI\u2019s Completion and Embedding endpoints with our Pinecone vector database to create a retrieval-augmented GQA system.\n#Query => .Embedding() => Vector DB => Context-enhanced Query => .Completion() => Response\nres = openai.Embedding.create(\n    input=[query],\n    engine=embed_model\n)\n\n# retrieve from Pinecone\nxq = res['data'][0]['embedding']\n\n# get relevant contexts (including the questions)\nres = index.query(xq, top_k=2, include_metadata=True)\n\nlimit = 3750\n\ndef retrieve(query):\n    res = openai.Embedding.create(\n        input=[query],\n        engine=embed_model\n    )\n\n    # retrieve from Pinecone\n    xq = res['data'][0]['embedding']\n\n    # get relevant contexts\n    res = index.query(xq, top_k=3, include_metadata=True)\n    contexts = [\n        x['metadata']['text'] for x in res['matches']\n    ]\n\n    # build our prompt with the retrieved contexts included\n    prompt_start = (\n        \"Answer the question based on the context below.\\n\\n\"+\n        \"Context:\\n\"\n    )\n    prompt_end = (\n        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n    )\n    # append contexts until hitting limit\n    for i in range(1, len(contexts)):\n        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n            prompt = (\n                prompt_start +\n                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n                prompt_end\n            )\n            break\n        elif i == len(contexts)-1:\n            prompt = (\n                prompt_start +\n                \"\\n\\n---\\n\\n\".join(contexts) +\n                prompt_end\n            )\n    return prompt\n\n\n# first we retrieve relevant items from Pinecone\nquery_with_contexts = retrieve(query)\nquery_with_contexts\n\n# then we complete the context-infused query\ncomplete(query_with_contexts)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"  jamescalam/youtube-transcriptions = Dataset({\n      features: ['title', 'published', 'url', 'video_id', 'channel_id', 'id', 'text', 'start', 'end'],\n      num_rows: 208619\n  })\n\n  data[0] = {'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n      'published': '2021-07-06 13:00:03 UTC',\n      'url': 'https://youtu.be/35Pdoyi6ZoQ',\n      'video_id': '35Pdoyi6ZoQ',\n      'channel_id': 'UCv83tO5cePwHMt1952IVVHw',\n      'id': '35Pdoyi6ZoQ-t0.0',\n      'text': 'Hi, welcome to the video.',\n      'start': 0.0,\n      'end': 9.36\n  }\n\n  new_data[0] = {'start': 0.0,\n      'end': 74.12,\n      'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n      'text': \"Hi, welcome to the video. So this is the fourth video in a Transformers from Scratch mini series. So if you haven't been following along, we've essentially covered what you can see on the screen. So we got some data. We built a tokenizer with it. And then we've set up our input pipeline ready to begin actually training our model, which is what we're going to cover in this video. So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data.\",\n      'id': '35Pdoyi6ZoQ-t0.0',\n      'url': 'https://youtu.be/35Pdoyi6ZoQ',\n      'published': '2021-07-06 13:00:03 UTC',\n      'channel_id': 'UCv83tO5cePwHMt1952IVVHw'\n  }\n\n  index.describe_index_stats() = {'dimension': 1536,\n      'index_fullness': 0.0,\n      'namespaces': {},\n      'total_vector_count': 0\n  }\n")))}c.isMDXComponent=!0}}]);