"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[24732],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>g});var a=n(67294);function s(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){s(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,s=function(e,t){if(null==e)return{};var n,a,s={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(s[n]=e[n]);return s}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(s[n]=e[n])}return s}var i=a.createContext({}),c=function(e){var t=a.useContext(i),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(i.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,s=e.mdxType,o=e.originalType,i=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),d=c(n),h=s,g=d["".concat(i,".").concat(h)]||d[h]||u[h]||o;return n?a.createElement(g,r(r({ref:t},p),{},{components:n})):a.createElement(g,r({ref:t},p))}));function g(e,t){var n=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var o=n.length,r=new Array(o);r[0]=h;var l={};for(var i in t)hasOwnProperty.call(t,i)&&(l[i]=t[i]);l.originalType=e,l[d]="string"==typeof e?e:s,r[1]=l;for(var c=2;c<o;c++)r[c]=n[c];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},66249:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>i,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var a=n(87462),s=(n(67294),n(3905));const o={},r="How Can the ELK Stack Be Used to Monitor PHP Apps?",l={unversionedId:"ElasticSearch/How Can the ELK Stack Be Used to Monitor PHP Apps",id:"ElasticSearch/How Can the ELK Stack Be Used to Monitor PHP Apps",title:"How Can the ELK Stack Be Used to Monitor PHP Apps?",description:"link//www.sitepoint.com/how-can-the-elk-stack-be-used-to-monitor-php-apps",source:"@site/docs/ElasticSearch/How Can the ELK Stack Be Used to Monitor PHP Apps.md",sourceDirName:"ElasticSearch",slug:"/ElasticSearch/How Can the ELK Stack Be Used to Monitor PHP Apps",permalink:"/notebook/docs/ElasticSearch/How Can the ELK Stack Be Used to Monitor PHP Apps",draft:!1,editUrl:"https://github.com/scheffershen/notebook/tree/main/docs/ElasticSearch/How Can the ELK Stack Be Used to Monitor PHP Apps.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"elasticsearch client php-api 2.0",permalink:"/notebook/docs/ElasticSearch/Guide/elasticsearch client php-api 2"},next:{title:"Comment installer et configurer Elasticsearch sur Ubuntu 18.04",permalink:"/notebook/docs/ElasticSearch/Installation/Comment installer et configurer Elasticsearch sur Ubuntu 18_04"}},i={},c=[{value:"Installing ELK",id:"installing-elk",level:2},{value:"Installing Logstash",id:"installing-logstash",level:2},{value:"Installing Kibana",id:"installing-kibana",level:2},{value:"Install Nginx",id:"install-nginx",level:2},{value:"Shipping Logs",id:"shipping-logs",level:2},{value:"Generate SSL Certificates",id:"generate-ssl-certificates",level:2},{value:"Option 1: IP Address",id:"option-1-ip-address",level:3},{value:"Configure Logstash",id:"configure-logstash",level:2}],p={toc:c};function d(e){let{components:t,...n}=e;return(0,s.kt)("wrapper",(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"how-can-the-elk-stack-be-used-to-monitor-php-apps"},"How Can the ELK Stack Be Used to Monitor PHP Apps?"),(0,s.kt)("p",null,"link: ",(0,s.kt)("a",{parentName:"p",href:"https://www.sitepoint.com/how-can-the-elk-stack-be-used-to-monitor-php-apps"},"https://www.sitepoint.com/how-can-the-elk-stack-be-used-to-monitor-php-apps")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://github.com/jmyoung/docker-elk"},"https://github.com/jmyoung/docker-elk")),(0,s.kt)("p",null,"Daniel Berman\nMay 11, 2016"),(0,s.kt)("p",null,"The ELK Stack (Elasticsearch, Logstash, and Kibana), "),(0,s.kt)("p",null,"ELK is a great way to centralize logs from multiple sources, identify correlations, and perform deep-data analysis."),(0,s.kt)("p",null,"Elasticsearch is a search-and-analytics engine based on Apache Lucene that allows users to search and analyze large amounts of data in almost real time."),(0,s.kt)("p",null,"Logstash can ingest and forward logs from anywhere to anywhere."),(0,s.kt)("p",null,"Kibana is the stack\u2019s pretty face \u2014 a user interface that allows you to query, visualize, and explore Elasticsearch data easily."),(0,s.kt)("p",null,"This article will describe how to set up the ELK Stack on a local development environment, ship web server logs (Apache logs in this case) into Elasticsearch using Logstash, and then analyze the data in Kibana."),(0,s.kt)("h2",{id:"installing-elk"},"Installing ELK"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"$ wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\n\n$ echo \"deb http://packages.elastic.co/elasticsearch/2.x/debian stable main\" | sudo tee -a /etc/apt/sources.list.d/elasticsearch-2.x.list\n\n$ sudo apt-get update && sudo apt-get install elasticsearch\n\n$ sudo nano /etc/elasticsearch/elasticsearch.yml\n\n    network.host: localhost\n\n$ sudo service elasticsearch restart\n\n$ sudo update-rc.d elasticsearch defaults 95 10\n\n$ sudo curl 'http://localhost:9200'\n")),(0,s.kt)("h2",{id:"installing-logstash"},"Installing Logstash"),(0,s.kt)("p",null,"Logstash, the \u201cL\u201d in the \u201cELK Stack\u201d, is used at the beginning of the log pipeline, ingesting and collecting data before sending it on to Elasticsearch."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},'$ echo "deb http://packages.elastic.co/logstash/2.2/debian stable main" | sudo tee -a /etc/apt/sources.list\n\n$ sudo apt-get update && sudo apt-get install logstash\n')),(0,s.kt)("h2",{id:"installing-kibana"},"Installing Kibana"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},'$ echo "deb http://packages.elastic.co/kibana/4.5/debian stable main" | sudo tee -a /etc/apt/sources.list\n\n$ sudo apt-get update && apt-get install kibana\n\n$ sudo nano /opt/kibana/config/kibana.yml\n\n    server.host: "localhost"\n\n$ sudo update-rc.d kibana defaults 96 9\n$ sudo service kibana start    \n')),(0,s.kt)("h2",{id:"install-nginx"},"Install Nginx"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"$ sudo apt-get install nginx apache2-utils\n$ sudo htpasswd -c /etc/nginx/htpasswd.users kibanaadmin\n")),(0,s.kt)("p",null,"kibanapass"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"$ sudo nano /etc/nginx/sites-available/default\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"    server {\n        listen 8080;\n\n        server_name example.com;\n\n        auth_basic \"Restricted Access\";\n        auth_basic_user_file /etc/nginx/htpasswd.users;\n\n        location / {\n            proxy_pass http://localhost:5601;\n            proxy_http_version 1.1;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection 'upgrade';\n            proxy_set_header Host $host;\n            proxy_cache_bypass $http_upgrade;        \n        }\n    }\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"$ sudo service nginx restart    \n")),(0,s.kt)("h2",{id:"shipping-logs"},"Shipping Logs"),(0,s.kt)("p",null,"Logstash configuration files are written in JSON format and reside in /etc/logstash/conf.d. The configuration consists of three plugin sections: input, filter, and output."),(0,s.kt)("p",null,"Create a configuration file called apache-logs.conf:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"$ sudo nano /etc/logstash/conf.d/apache-logs.conf\n")),(0,s.kt)("p",null,"/var/www/medflix.universalmedica.com/medflixs_access.log"),(0,s.kt)("p",null,"/etc/logstash/conf.d/apache-logs.conf"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-conf"},'input {\n    file {\n        path => "/var/www/medflix.universalmedica.com/medflixs_access.log"\n        type => "apache-access"\n    }\n}   \nfilter {\n  if [type] == "apache-access" {\n    grok {\n      match => { "message" => "%{COMBINEDAPACHELOG}" }\n    }\n  }\n}\noutput {\n    elasticsearch {}\n}\n')),(0,s.kt)("p",null,"That\u2019s it. Once you\u2019re done, start Logstash with the new configuration:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"$ sudo service logstash configtest\n\n$ sudo service logstash restart\n$ sudo update-rc.d logstash defaults 96 9\n")),(0,s.kt)("p",null,"/opt/logstash/bin/logstash -f /etc/logstash/conf.d/apache-logs.conf"),(0,s.kt)("h2",{id:"generate-ssl-certificates"},"Generate SSL Certificates"),(0,s.kt)("p",null,"Since we are going to use Filebeat to ship logs from our Client Servers to our ELK Server, we need to create an SSL certificate and key pair. The certificate is used by Filebeat to verify the identity of ELK Server. "),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"$ sudo mkdir -p /etc/pki/tls/certs\n$ sudo mkdir /etc/pki/tls/private\n")),(0,s.kt)("h3",{id:"option-1-ip-address"},"Option 1: IP Address"),(0,s.kt)("h2",{id:"configure-logstash"},"Configure Logstash"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"$ sudo nano /etc/logstash/conf.d/02-beats-input.conf\n")),(0,s.kt)("p",null,"/etc/logstash/conf.d/02-beats-input.conf"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-conf"},'    input {\n      beats {\n        port => 5044\n        ssl => true\n        ssl_certificate => "/etc/pki/tls/certs/logstash-forwarder.crt"\n        ssl_key => "/etc/pki/tls/private/logstash-forwarder.key"\n      }\n    }\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"$ sudo vi /etc/logstash/conf.d/10-syslog-filter.conf\n")),(0,s.kt)("p",null,"/etc/logstash/conf.d/10-syslog-filter.conf"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-conf"},'    filter {\n      if [type] == "syslog" {\n        grok {\n          match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\\[%{POSINT:syslog_pid}\\])?: %{GREEDYDATA:syslog_message}" }\n          add_field => [ "received_at", "%{@timestamp}" ]\n          add_field => [ "received_from", "%{host}" ]\n        }\n        syslog_pri { }\n        date {\n          match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]\n        }\n      }\n    }\n')))}d.isMDXComponent=!0}}]);