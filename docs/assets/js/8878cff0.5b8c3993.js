"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3896],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>m});var o=t(7294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function s(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,o,i=function(e,n){if(null==e)return{};var t,o,i={},a=Object.keys(e);for(o=0;o<a.length;o++)t=a[o],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)t=a[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var r=o.createContext({}),c=function(e){var n=o.useContext(r),t=n;return e&&(t="function"==typeof e?e(n):s(s({},n),e)),t},p=function(e){var n=c(e.components);return o.createElement(r.Provider,{value:n},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},g=o.forwardRef((function(e,n){var t=e.components,i=e.mdxType,a=e.originalType,r=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),d=c(t),g=i,m=d["".concat(r,".").concat(g)]||d[g]||u[g]||a;return t?o.createElement(m,s(s({ref:n},p),{},{components:t})):o.createElement(m,s({ref:n},p))}));function m(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var a=t.length,s=new Array(a);s[0]=g;var l={};for(var r in n)hasOwnProperty.call(n,r)&&(l[r]=n[r]);l.originalType=e,l[d]="string"==typeof e?e:i,s[1]=l;for(var c=2;c<a;c++)s[c]=t[c];return o.createElement.apply(null,s)}return o.createElement.apply(null,t)}g.displayName="MDXCreateElement"},7551:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>s,default:()=>d,frontMatter:()=>a,metadata:()=>l,toc:()=>c});var o=t(7462),i=(t(7294),t(3905));const a={},s="using-google-cloud-machine-learning-apis-programmatically-in-python",l={unversionedId:"Google API/using-google-cloud-machine-learning-apis-programmatically-in-python",id:"Google API/using-google-cloud-machine-learning-apis-programmatically-in-python",title:"using-google-cloud-machine-learning-apis-programmatically-in-python",description:"https://towardsdatascience.com/using-google-cloud-machine-learning-apis-programmatically-in-python-part-1-430f608af6a5",source:"@site/docs/Google API/using-google-cloud-machine-learning-apis-programmatically-in-python.md",sourceDirName:"Google API",slug:"/Google API/using-google-cloud-machine-learning-apis-programmatically-in-python",permalink:"/notebook/docs/Google API/using-google-cloud-machine-learning-apis-programmatically-in-python",draft:!1,editUrl:"https://github.com/scheffershen/notebook/tree/main/docs/Google API/using-google-cloud-machine-learning-apis-programmatically-in-python.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"google-search-tips",permalink:"/notebook/docs/Google API/google-search-tips"},next:{title:"HTML",permalink:"/notebook/docs/category/html"}},r={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Google Cloud ML APIs",id:"google-cloud-ml-apis",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"1. Vision API",id:"1-vision-api",level:2},{value:"Label Detection",id:"label-detection",level:2},{value:"Face Detection",id:"face-detection",level:2},{value:"Landmark Detection",id:"landmark-detection",level:2},{value:"Logo Detection",id:"logo-detection",level:2},{value:"Web Detection",id:"web-detection",level:2},{value:"2. Speech to Text API",id:"2-speech-to-text-api",level:2},{value:"3. Text to Speech API",id:"3-text-to-speech-api",level:2},{value:"1. Translation API",id:"1-translation-api",level:2},{value:"Language Detection",id:"language-detection",level:2},{value:"Video Intelligence API",id:"video-intelligence-api",level:2},{value:"Shot Change Detection",id:"shot-change-detection",level:2},{value:"Speech Transcription",id:"speech-transcription",level:2},{value:"Label Detection",id:"label-detection-1",level:2},{value:"Logo Detection",id:"logo-detection-1",level:2},{value:"People Detection",id:"people-detection",level:2},{value:"Face Detection",id:"face-detection-1",level:2},{value:"Natural Language API",id:"natural-language-api",level:2},{value:"Entity Analysis",id:"entity-analysis",level:2},{value:"Sentiment Analysis",id:"sentiment-analysis",level:2},{value:"Syntax Analysis",id:"syntax-analysis",level:2},{value:"Text Classification",id:"text-classification",level:2}],p={toc:c};function d(e){let{components:n,...t}=e;return(0,i.kt)("wrapper",(0,o.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"using-google-cloud-machine-learning-apis-programmatically-in-python"},"using-google-cloud-machine-learning-apis-programmatically-in-python"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://towardsdatascience.com/using-google-cloud-machine-learning-apis-programmatically-in-python-part-1-430f608af6a5"},"https://towardsdatascience.com/using-google-cloud-machine-learning-apis-programmatically-in-python-part-1-430f608af6a5"),"\nFeb 7, 2022"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/Subrahmanyajoshi/Google-Cloud-Machine-Learning-APIs"},"https://github.com/Subrahmanyajoshi/Google-Cloud-Machine-Learning-APIs")),(0,i.kt)("h2",{id:"introduction"},"Introduction"),(0,i.kt)("p",null,"Developing a machine learning system from scratch is not an easy task. It\u2019s extremely time and resource consuming. The development of an end-to-end machine learning system consists of various different stages like:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Problem understanding"),(0,i.kt)("li",{parentName:"ol"},"Data fetching"),(0,i.kt)("li",{parentName:"ol"},"Data cleaning"),(0,i.kt)("li",{parentName:"ol"},"Data labeling"),(0,i.kt)("li",{parentName:"ol"},"Feature selection"),(0,i.kt)("li",{parentName:"ol"},"Model selection, training, and hyperparameter tuning"),(0,i.kt)("li",{parentName:"ol"},"Testing"),(0,i.kt)("li",{parentName:"ol"},"Deployment and maintenance")),(0,i.kt)("p",null,"For general use cases, using already available Machine Learning APIs makes more sense and also saves time and resources."),(0,i.kt)("p",null,"Google Cloud provides highly accurate, fully managed APIs which solve most of the common machine learning problems. "),(0,i.kt)("p",null,"These APIs are also trained on huge datasets and results are much more accurate than what we would get if we build and train a custom model ourselves."),(0,i.kt)("h2",{id:"google-cloud-ml-apis"},"Google Cloud ML APIs"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Vision API"),(0,i.kt)("li",{parentName:"ul"},"Speech to Text API"),(0,i.kt)("li",{parentName:"ul"},"Text to Speech API"),(0,i.kt)("li",{parentName:"ul"},"Natural Language API"),(0,i.kt)("li",{parentName:"ul"},"Video Intelligence API"),(0,i.kt)("li",{parentName:"ul"},"Translation API")),(0,i.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://cloud.google.com/vertex-ai/docs/workbench/user-managed/create-new"},"Create a Google Cloud AI notebooks instance"),", No need to create a very powerful instance. ",(0,i.kt)("inlineCode",{parentName:"li"},"n1-standard-2")," instances would be fine."),(0,i.kt)("li",{parentName:"ul"},"Install the following python packages through pip in the notebook instance",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"google-cloud-vision==2.5.0"),(0,i.kt)("li",{parentName:"ul"},"google-cloud-language==2.3.0"),(0,i.kt)("li",{parentName:"ul"},"google-cloud-translate==3.6.0"),(0,i.kt)("li",{parentName:"ul"},"google-cloud-speech==2.11.0"),(0,i.kt)("li",{parentName:"ul"},"google-cloud-texttospeech==2.10.0"),(0,i.kt)("li",{parentName:"ul"},"google-cloud-videointelligence==2.5.0"))),(0,i.kt)("li",{parentName:"ul"},"Create a google cloud service account and its associated key by running commands given below in the Google Cloud shell instance.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'# Set PROJECT_ID environment variable \nexport PROJECT_ID=<project-id>\n\n# Create a google cloud service account \ngcloud iam service-accounts create my-api-sa --display-name "api account"\n\n# Create the key\ngcloud iam service-accounts keys create ~/key.json --iam-account my-api-sa@${PROJECT_ID}.iam.gserviceaccount.com\n')),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://console.cloud.google.com/apis/library?project=text-analysis-323506&pli=1"},"APIs need to be enabled before they are used"))),(0,i.kt)("h2",{id:"1-vision-api"},"1. Vision API"),(0,i.kt)("p",null,"Vision API has features to solve most of the common image processing problems."),(0,i.kt)("p",null,"List of Vision API features I have experimented with:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Text Detection")),(0,i.kt)("p",null,"This feature uses OCR to detect all kinds of texts from an input image. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import os\nimport io\n\nfrom IPython.display import Image\nfrom google.cloud import vision\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.path.abspath(\"/home/jupyter/key.json\")\n\n# Create a Google Cloud Vision api client object\nvision_client = vision.ImageAnnotatorClient()\nimg_path = './data/gsuite.jpg'\nImage(filename=img_path, width=700, height=500) \n\n# Create a vision image object\nwith io.open(img_path, 'rb') as image_file:\n    content = image_file.read()\n        \nimage_object = vision.Image(content=content)\n\n# Send a request to cloud vision api client to detect texts in input image\nresponse = vision_client.text_detection(image=image_object)\n\n# Results\nresponse.text_annotations[0].description.replace('\\n', '\\n ')\n\n# 'Google Cloud\\n All of us are smarter\\n than one of us.\\n Introducing G Suite, intelligent apps made for working together\\n and building a brighter future for your business.\\n google.com/gsuite\\n G Suite MB AE\\n h\u7530\u7530\\n \u7530 \u7530\u7530\\n '\n")),(0,i.kt)("h2",{id:"label-detection"},"Label Detection"),(0,i.kt)("p",null,"This feature labels the input image. It basically detects and returns a list of everything it finds in the queried image."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import os\nimport io\n\nfrom IPython.display import Image\nfrom google.cloud import vision\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath("/home/jupyter/key.json")\nvision_client = vision.ImageAnnotatorClient()\n\n# Create a Google Cloud Vision api client object\nvision_client = vision.ImageAnnotatorClient()\nimg_path = \'./data/dog.jpg\'\nImage(filename=img_path, width=400, height=400) \n\n# Create a vision image object\nwith io.open(img_path, \'rb\') as image_file:\n    content = image_file.read()\n        \nimage_object = vision.Image(content=content)\n\n# Send a request to cloud vision api client to detect labels in input image\nresponse = vision_client.label_detection(image=image_object)\n\n# Results\n# Response with the highest confidence\nprint(f"Label: {response.label_annotations[0].description}, Confidence: {response.label_annotations[0].score}")\n\n# Label: Dog, Confidence: 0.9537228941917419\n\n# Other responses\nfor responses in response.label_annotations:\n    print(f"Label: {responses.description}, Confidence: {responses.score}")\n\n# Label: Dog, Confidence: 0.9537228941917419\n# Label: Dog breed, Confidence: 0.9165488481521606\n# Label: Carnivore, Confidence: 0.8981429934501648\n# Label: Disc dog, Confidence: 0.8752759099006653\n# Label: Flying disc, Confidence: 0.8572475910186768\n# Label: Frisbee games, Confidence: 0.8473882079124451\n# Label: Fawn, Confidence: 0.8161194324493408\n# Label: Companion dog, Confidence: 0.8147048950195312\n# Label: Tail, Confidence: 0.7616308331489563\n# Label: Snout, Confidence: 0.7511301040649414\n')),(0,i.kt)("h2",{id:"face-detection"},"Face Detection"),(0,i.kt)("p",null,"This feature detects all the faces in the queried image. Returns coordinates of bounding boxes for detected faces along with sentiments of each detected face."),(0,i.kt)("h2",{id:"landmark-detection"},"Landmark Detection"),(0,i.kt)("p",null,"This feature detects the place in the queried image. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import os\nimport io\n\nfrom IPython.display import Image\nfrom google.cloud import vision\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath("/home/jupyter/key.json")\n\n# Create a Google Cloud Vision api client object\nvision_client = vision.ImageAnnotatorClient()\nimg_path = \'./data/landmark.jpg\'\nImage(filename=img_path, width=400, height=400) \n\n# Create a vision image object\nwith io.open(img_path, \'rb\') as image_file:\n    content = image_file.read()\n        \nimage_object = vision.Image(content=content)\n\n# Send a request to cloud vision api client to detect landmarks in input image\nresponse = vision_client.landmark_detection(image=image_object)\n\n# Results\nprint("Detected Landmark: ", response.landmark_annotations[0].description)\nprint("Score: ", response.landmark_annotations[0].score)\n\n# Detected Landmark:  Times Square\n# Score:  0.8622481822967529\n')),(0,i.kt)("h2",{id:"logo-detection"},"Logo Detection"),(0,i.kt)("p",null,"This feature detects popular logos in the queried image."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import os\nimport io\n\nfrom IPython.display import Image\nfrom google.cloud import vision\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.path.abspath(\"/home/jupyter/key.json\")\n\n# Create a Google Cloud Vision api client object\nvision_client = vision.ImageAnnotatorClient()\nimg_path = './data/logos.jpg'\nImage(filename=img_path, width=400, height=400) \n\n# Create a vision image object\nwith io.open(img_path, 'rb') as image_file:\n    content = image_file.read()\n        \nimage_object = vision.Image(content=content)\n\n# Send a request to cloud vision api client to detect logos in input image\nresponse = vision_client.logo_detection(image=image_object)\n\n# Results\nfor logo in response.logo_annotations:\n    print(logo.description)\n\n# Royal Dutch Shell\n# Target Corporation\n# Nike\n# Apple Inc.\n# McDonald's\n# NBC News\n# World Wide Fund for Nature\n# Mercedes-Benz\n")),(0,i.kt)("h2",{id:"web-detection"},"Web Detection"),(0,i.kt)("p",null,"This feature retrieves images over the internet, which are similar (fully, partially, or visually) to the queried image."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import os\nimport io\n\nfrom IPython.display import Image\nfrom google.cloud import vision\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath("/home/jupyter/key.json")\n\n# Create a Google Cloud Vision api client object\nvision_client = vision.ImageAnnotatorClient()\nimg_path = \'./data/dog.jpg\'\nImage(filename=img_path, width=400, height=400) \n\n# Create a vision image object\nwith io.open(img_path, \'rb\') as image_file:\n    content = image_file.read()\n        \nimage_object = vision.Image(content=content)\n\n# Send a request to cloud vision api client to detect similar images as input image\nresponse = vision_client.web_detection(image=image_object)\n\n# Full Matching Images\n# response.web_detection.full_matching_images \n\n# [url: "https://img.gospodari.com/ckfinder/userfiles/images/3525.jpg"\n# , url: "https://funart.pro/uploads/posts/2021-07/1627618416_14-funart-pro-p-sobaka-v-polete-zhivotnie-krasivo-foto-16.jpg"\n# , url: "https://cameralabs.org/media/lab19/10/29-2/fotograf-klaudio-pikkoli-6.jpg"\n# , url: "https://lookaside.fbsbx.com/lookaside/crawler/media/?media_id=146502367480258"\n# , url: "https://i.pinimg.com/originals/b1/fb/34/b1fb34fde9e0da61310f1a73d4e1de71.jpg"\n# , url: "https://1.bp.blogspot.com/-nGeUlFfslaU/XdVncGu_5RI/AAAAAAAAa08/BQUf30LmIWAjIbpjsQjwlUPlzCdjtC0agCLcBGAsYHQ/s1600/Wonderful%2BPictures%2Bof%2BSuper%2BFlying%2BDogs%2Bby%2BClaudio%2BPiccoli%2B%25288%2529.jpg"\n# , url: "https://millionstatusov.ru/pic/statpic/all8/5e20bda8e78a6.jpg"\n# , url: "https://i.pinimg.com/736x/b1/fb/34/b1fb34fde9e0da61310f1a73d4e1de71.jpg"\n# , url: "https://i.guim.co.uk/img/media/339bbeb00e74a94fce675db75db6b22945a1dc13/0_0_3525_2349/master/3525.jpg?width=700&quality=85&auto=format&fit=max&s=8c3a8add56e61c2b55a5d75023a88329"\n# , url: "https://i.pinimg.com/474x/b1/fb/34/b1fb34fde9e0da61310f1a73d4e1de71.jpg"\n# ]\n\n# Partially Matching Images\n# response.web_detection.partial_matching_images\n\n# [url: "http://ocdn.eu/images/pulscms/OWY7MDA_/55b440ca49f66cfea70203e352cb71ae.jpg"\n# , url: "https://img.wprost.pl/_thumb/f8/48/c9f3c054a99c80e1c5xx57e5aeeb.jpeg"\n# , url: "https://cdnpt01.viewbug.com/media/mediafiles/2019/01/21/82934242_widepreview400.jpg"\n# , url: "https://seuppcdn01.1x.com/images/user/90d7cebdc484c375d0e26eebb688190d-ld.jpg"\n# , url: "https://static.wixstatic.com/media/40ce9a_0200a36bab4940ec88f7657937cfdd06~mv2.jpg/v1/fill/w_250,h_166,al_c,q_90/40ce9a_0200a36bab4940ec88f7657937cfdd06~mv2.jpg"\n# , url: "https://c1.titrebartar.com//images/news/a60/1398/08/12/1572783357_H6cE0.jpg"\n# , url: "https://lookaside.fbsbx.com/lookaside/crawler/media/?media_id=484637275718383"\n# , url: "https://seuppcdn01.1x.com/images/user/90d7cebdc484c375d0e26eebb688190d-hd2.jpg"\n# , url: "https://static3.55online.news/thumbnail/zhkNmZhOTM3M/odJ5qJgIOksnDoJdBenUlE48u50d9ntalnDkvdNYgn497B9enj5kxs-zhltB_746/zhkNmZhOTM3M.jpg"\n# , url: "https://lookaside.fbsbx.com/lookaside/crawler/media/?media_id=136158531125831"\n# ]\n\n# Visually Similar Images\n# response.web_detection.visually_similar_images\n\n# [url: "https://images.cm.archant.co.uk/resource/image/8626336/landscape_ratio16x9/448/252/1b199d83bf5d3917eaf6595b1819c46/eh/xxx-jumping-dog-1.jpg"\n# , url: "https://www.awf.org/sites/default/files/SpeciesPage_AfricanWildDog01_Hero.jpg"\n# , url: "https://imagesvc.meredithcorp.io/v3/mm/image?url=https%3A%2F%2Fstatic.onecms.io%2Fwp-content%2Fuploads%2Fsites%2F47%2F2020%2F11%2F03%2Fcavalier-king-charles-spaniel-1352518937-2000.jpg"\n# , url: "https://www.unley.sa.gov.au/files/assets/public/pets/dog-1.jpg?w=1200"\n# , url: "https://static.onecms.io/wp-content/uploads/sites/47/2020/06/28/Catahoula-Leopard-Dog-lying-down-627457592-2000.jpg"\n# , url: "https://vetstreet.brightspotcdn.com/dims4/default/54186d0/2147483647/thumbnail/590x420/quality/90/?url=https%3A%2F%2Fvetstreet-brightspot.s3.amazonaws.com%2F40%2F58%2F3bc5c01c4cdb8a0581681831faa9%2Fgreat-dane-shaking-paw-thinkstockphotos-522650067-590.jpg"\n# , url: "https://imagesvc.meredithcorp.io/v3/mm/image?url=https%3A%2F%2Fstatic.onecms.io%2Fwp-content%2Fuploads%2Fsites%2F47%2F2021%2F03%2F25%2Fdoberman-pinscher-red-collar-1100812121-2000.jpg"\n# , url: "https://i.natgeofe.com/n/187b3223-0b45-4aa5-ae5c-24793dd2d6cb/01-german-shepherd-coronavirus-bwtkdt_16x9.jpg?w=1200"\n# ]\n\n')),(0,i.kt)("h2",{id:"2-speech-to-text-api"},"2. Speech to Text API"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://console.cloud.google.com/marketplace/product/google/speech.googleapis.com"},"Google Cloud Speech to Text API is used for speech transcription.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import os\nimport io\n\nfrom IPython.display import Audio\nfrom google.cloud import speech_v1\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath("/home/jupyter/key.json")\n\n# Create a Google Cloud Vision api client object\nspeech_client = speech_v1.SpeechClient()\nspeech_file = \'./data/recording.flac\'\nAudio(speech_file)\n\n# Read the audio file\nwith io.open(speech_file, "rb") as audio_file:\n    content = audio_file.read()\n        \naudio = speech_v1.RecognitionAudio(content=content)\n\nspeech_config =  {"language_code": "en-In",\n                  "encoding": speech_v1.RecognitionConfig.AudioEncoding.FLAC,\n                 \'audio_channel_count\' : 2 \n                 }\n\nresponse = speech_client.recognize(config=speech_config, audio=audio)\n\n# Transcribed speech\n# response.results[0].alternatives[0].transcript\n\n# \'hi I am Subramaniam Joshi I am a data scientist at Hewlett Packard enterprise I love machine learning and artificial intelligence\'\n\n')),(0,i.kt)("h2",{id:"3-text-to-speech-api"},"3. Text to Speech API"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://console.cloud.google.com/marketplace/product/google/texttospeech.googleapis.com?project=flutter-217514"},"This API is used for the conversion of text to speech.")),(0,i.kt)("p",null,"While converting text to speech, this API provides a lot of different voice configurations."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import os\nimport io\n\nfrom IPython.display import Audio\nfrom google.cloud import texttospeech_v1\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath("/home/jupyter/key.json")\n\n# Create a text to speech client object\ntts_client = texttospeech_v1.TextToSpeechClient()\n\ntext = "hi I am Subramaniam Joshi I am a data scientist at Hewlett Packard enterprise I love machine learning and artificial intelligence"\n\ninput_text = texttospeech_v1.SynthesisInput(text=text)\n\n# Following cell lists all available voice configurations\ntts_client.list_voices()\n\n# Here i\'m using an en-In configuration with name en-IN-Wavenet-D\nvoice = texttospeech_v1.VoiceSelectionParams(\n        language_code="en-In",\n        name="en-IN-Wavenet-D")\n\n# Reponse audio configurations\naudio_config = texttospeech_v1.AudioConfig(\n        audio_encoding=texttospeech_v1.AudioEncoding.MP3)\n\nresponse = tts_client.synthesize_speech(request={"input": input_text, "voice": voice, "audio_config": audio_config})\n\n# Response\nAudio(response.audio_content)\n\n')),(0,i.kt)("h2",{id:"1-translation-api"},"1. Translation API"),(0,i.kt)("p",null,"As the name itself suggests, this API is mainly used for text translation.",(0,i.kt)("a",{parentName:"p",href:"https://console.cloud.google.com/marketplace/product/google/translate.googleapis.com?project=text-analysis-323506"},"\nCloud Translation API")),(0,i.kt)("p",null,"I\u2019m using Translation API to translate a French article to English and Japanese."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import os\n\nfrom google.cloud import translate_v2\n\nos.environ[\"PROJECT_ID\"] = \"text-analysis-323506\"\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.path.abspath(\"/home/jupyter/key.json\")\n\n# Create a Google Cloud translate api client object\ntranslate_client = translate_v2.Client()\n\n# Text file localted inside data folder contains 'french_article.txt' which contains a new artical in french.\n\nfile_path = './data/french_article.txt'\n\n# Original article in French\ntext_lines = []\nwith open(file_path, 'r') as fstream:\n    lines = fstream.readlines()\n    for line in lines:\n        text_lines.append(line.strip())\n        print(line.strip())\n\n# D\xc9CRYPTAGESElectronique, bois, plastiques : les entreprises de la plan\xe8te sont confront\xe9es \xe0 un manque de mati\xe8res et de composants pour r\xe9pondre \xe0 la demande. Voici pourquoi.\n\n# Les prix du gaz et de l\u2019\xe9lectricit\xe9 ne sont pas les seuls \xe0 s\u2019envoler depuis quelques mois. Ceux de nombreuses mati\xe8res premi\xe8res et composants alimentant les cha\xeenes de production des usines du monde entier grimpent aussi, ralentissant la production et la livraison de biens de consommation. Le Monde fait le point sur ces p\xe9nuries.\n\n# Qu\u2019est-ce qu\u2019une p\xe9nurie ?\n# On parle de p\xe9nurie lors d\u2019un \xe9tat de manque partiel ou total de denr\xe9es ou de marchandises, \xe0 l\u2019\xe9chelle d\u2019un territoire ou d\u2019un groupe de personnes. D\u2019un point de vue \xe9conomique, la p\xe9nurie d\xe9signe le moment o\xf9 la demande pour une marchandise est sup\xe9rieure \xe0 l\u2019offre.\n\n# Celles-ci peuvent avoir des causes diverses, d\u2019ordre climatique (mauvaise r\xe9colte), politique (situation de guerre, blocus), \xe9conomique (insuffisance de production) ou encore purement sp\xe9culatif : la peur de manquer entra\xeenant une ru\xe9e sur un ou plusieurs produits.\n\n# Au d\xe9but de la crise sanitaire due au Covid-19, le confinement a provoqu\xe9 une fr\xe9n\xe9sie d\u2019achat de p\xe2tes ou de papier toilettes, qui s\u2019est accentu\xe9e par les images de rayons vides, cr\xe9ant ainsi la p\xe9nurie dans les supermarch\xe9s.\n\n# Pourquoi y a-t-il des p\xe9nuries actuellement ?\n# \xb7 Parce que la demande a repris bien plus fortement que pr\xe9vu\n\n# Apr\xe8s ces mises \xe0 l\u2019arr\xeat g\xe9n\xe9ralis\xe9es et successives des \xe9conomies nationales au cours des confinements en 2020 et 2021, la demande de produits manufactur\xe9s et de mati\xe8res premi\xe8res a repris fortement, entra\xeenant des tensions par rapport \xe0 une offre inf\xe9rieure \xe0 la demande.\n\n# Le graphique ci-dessous montre clairement les d\xe9ficits d\u2019approvisionnement rencontr\xe9s depuis 2020. Le niveau le plus bas de l\u2019indice a \xe9t\xe9 atteint au moment du premier confinement. Les d\xe9lais d\u2019approvisionnement se sont bri\xe8vement raccourcis \xe0 l\u2019\xe9t\xe9 2020, avant de s\u2019allonger durablement jusqu\u2019\xe0 aujourd\u2019hui.\n\n# Translation to English\n\ntarget_language_code = 'en'\n\nresponses = translate_client.translate(text_lines, target_language=target_language_code)\n\nwith open('./results/english_translation.txt', 'w') as fstream:\n    for response in responses:\n        w_resposne = response.get('translatedText')\n        fstream.write(w_resposne + '\\n')\n        print(w_resposne)\n\n# DECRYPTIONElectronics, wood, plastics: companies around the world are facing a shortage of materials and components to meet demand. Here&#39;s why.\n\n# Gas and electricity prices are not the only ones to soar in recent months. Those of many raw materials and components feeding the production lines of factories around the world are also climbing, slowing the production and delivery of consumer goods. Le Monde takes stock of these shortages.\n\n# What is a shortage?\n# We speak of shortage when there is a partial or total lack of foodstuffs or goods, on the scale of a territory or a group of people. From an economic point of view, scarcity refers to when the demand for a commodity exceeds the supply.\n\n# These can have various causes, of a climatic order (poor harvest), political (war situation, blockade), economic (insufficiency of production) or even purely speculative: the fear of shortage leading to a rush on one or more products .\n\n# At the start of the health crisis due to Covid-19, the confinement caused a frenzy of purchases of pasta or toilet paper, which was accentuated by the images of empty shelves, thus creating a shortage in supermarkets.\n\n# Why are there current shortages?\n# Because demand has picked up much more strongly than expected\n\n# After these widespread and successive shutdowns of national economies during the confinements in 2020 and 2021, the demand for manufactured goods and raw materials has recovered strongly, leading to tensions in relation to a lower supply than demand.\n\n# The graph below clearly shows the supply shortages encountered since 2020. The lowest level of the index was reached at the time of the first confinement. Supply times shortened briefly in the summer of 2020, before lengthening sustainably until today.\n\n# Translation to Japanese\ntarget_language_code = 'ja'\n\nresponses = translate_client.translate(text_lines, target_language=target_language_code)\n\nwith open('./results/japanese_translation.txt', 'w') as fstream:\n    for response in responses:\n        w_resposne = response.get('translatedText')\n        fstream.write(w_resposne + '\\n')\n        print(w_resposne)\n\n# \u8aac\u660e\u96fb\u5b50\u6a5f\u5668\u3001\u6728\u6750\u3001\u30d7\u30e9\u30b9\u30c1\u30c3\u30af\uff1a\u4e16\u754c\u4e2d\u306e\u4f01\u696d\u306f\u3001\u9700\u8981\u3092\u6e80\u305f\u3059\u305f\u3081\u306e\u6750\u6599\u3068\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u4e0d\u8db3\u306b\u76f4\u9762\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u304c\u7406\u7531\u3067\u3059\u3002\n\n# \u3053\u3053\u6570\u30f6\u6708\u3067\u9ad8\u9a30\u3057\u3066\u3044\u308b\u306e\u306f\u30ac\u30b9\u3068\u96fb\u6c17\u306e\u4fa1\u683c\u3060\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u4e16\u754c\u4e2d\u306e\u5de5\u5834\u306e\u751f\u7523\u30e9\u30a4\u30f3\u306b\u4f9b\u7d66\u3055\u308c\u3066\u3044\u308b\u591a\u304f\u306e\u539f\u6750\u6599\u3084\u90e8\u54c1\u3082\u5897\u52a0\u3057\u3066\u304a\u308a\u3001\u6d88\u8cbb\u8ca1\u306e\u751f\u7523\u3068\u914d\u9001\u304c\u9045\u308c\u3066\u3044\u307e\u3059\u3002\u30eb\u30e2\u30f3\u30c9\u306f\u3053\u308c\u3089\u306e\u4e0d\u8db3\u3092\u88dc\u3044\u307e\u3059\u3002\n\n# \u4e0d\u8db3\u3068\u306f\u4f55\u3067\u3059\u304b\uff1f\n# \u9818\u571f\u3084\u4eba\u3005\u306e\u30b0\u30eb\u30fc\u30d7\u306e\u898f\u6a21\u3067\u3001\u98df\u6599\u3084\u5546\u54c1\u304c\u90e8\u5206\u7684\u307e\u305f\u306f\u5168\u4f53\u7684\u306b\u4e0d\u8db3\u3057\u3066\u3044\u308b\u5834\u5408\u3001\u79c1\u305f\u3061\u306f\u4e0d\u8db3\u306b\u3064\u3044\u3066\u8a71\u3057\u307e\u3059\u3002\u7d4c\u6e08\u7684\u89b3\u70b9\u304b\u3089\u3001\u5e0c\u5c11\u6027\u3068\u306f\u3001\u5546\u54c1\u306e\u9700\u8981\u304c\u4f9b\u7d66\u3092\u4e0a\u56de\u3063\u305f\u5834\u5408\u3092\u6307\u3057\u307e\u3059\u3002\n\n# \u3053\u308c\u3089\u306b\u306f\u3001\u6c17\u5019\u79e9\u5e8f\uff08\u4e0d\u4f5c\uff09\u3001\u653f\u6cbb\u7684\uff08\u6226\u4e89\u72b6\u6cc1\u3001\u5c01\u9396\uff09\u3001\u7d4c\u6e08\u7684\uff08\u751f\u7523\u4e0d\u8db3\uff09\u3001\u3055\u3089\u306b\u306f\u7d14\u7c8b\u306b\u6295\u6a5f\u7684\u306a\u3082\u306e\u306a\u3069\u3001\u3055\u307e\u3056\u307e\u306a\u539f\u56e0\u304c\u8003\u3048\u3089\u308c\u307e\u3059\u3002\u4e0d\u8db3\u306e\u6050\u308c\u304c1\u3064\u4ee5\u4e0a\u306e\u88fd\u54c1\u306e\u6025\u5897\u306b\u3064\u306a\u304c\u308a\u307e\u3059\u3002\n\n# Covid-19\u306b\u3088\u308b\u5065\u5eb7\u5371\u6a5f\u306e\u958b\u59cb\u6642\u306b\u3001\u9589\u3058\u8fbc\u3081\u306f\u30d1\u30b9\u30bf\u3084\u30c8\u30a4\u30ec\u30c3\u30c8\u30da\u30fc\u30d1\u30fc\u306e\u8cfc\u5165\u306e\u71b1\u72c2\u3092\u5f15\u304d\u8d77\u3053\u3057\u3001\u305d\u308c\u306f\u7a7a\u306e\u68da\u306e\u30a4\u30e1\u30fc\u30b8\u306b\u3088\u3063\u3066\u5f37\u8abf\u3055\u308c\u3001\u30b9\u30fc\u30d1\u30fc\u30de\u30fc\u30b1\u30c3\u30c8\u3067\u4e0d\u8db3\u3092\u5f15\u304d\u8d77\u3053\u3057\u307e\u3057\u305f\u3002\n\n# \u306a\u305c\u73fe\u5728\u4e0d\u8db3\u3057\u3066\u3044\u308b\u306e\u3067\u3059\u304b\uff1f\n# \u9700\u8981\u304c\u4e88\u60f3\u3088\u308a\u3082\u306f\u308b\u304b\u306b\u5f37\u304f\u56de\u5fa9\u3057\u305f\u305f\u3081\n\n# 2020\u5e74\u30682021\u5e74\u306e\u76e3\u7981\u671f\u9593\u4e2d\u306e\u3053\u308c\u3089\u306e\u5e83\u7bc4\u56f2\u306b\u308f\u305f\u308b\u9023\u7d9a\u7684\u306a\u56fd\u5bb6\u7d4c\u6e08\u306e\u9589\u9396\u306e\u5f8c\u3001\u5de5\u696d\u88fd\u54c1\u3068\u539f\u6750\u6599\u306e\u9700\u8981\u306f\u529b\u5f37\u304f\u56de\u5fa9\u3057\u3001\u9700\u8981\u3088\u308a\u3082\u4f9b\u7d66\u304c\u5c11\u306a\u3044\u3053\u3068\u306b\u95a2\u9023\u3059\u308b\u7dca\u5f35\u306b\u3064\u306a\u304c\u3063\u305f\u3002\n\n# \u4e0b\u306e\u30b0\u30e9\u30d5\u306f\u30012020\u5e74\u4ee5\u964d\u306b\u767a\u751f\u3057\u305f\u4f9b\u7d66\u4e0d\u8db3\u3092\u660e\u78ba\u306b\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u6700\u521d\u306e\u76e3\u7981\u6642\u306b\u6307\u6570\u306e\u6700\u4f4e\u30ec\u30d9\u30eb\u306b\u9054\u3057\u307e\u3057\u305f\u3002\u4f9b\u7d66\u6642\u9593\u306f2020\u5e74\u306e\u590f\u306b\u4e00\u6642\u7684\u306b\u77ed\u7e2e\u3055\u308c\u305f\u5f8c\u3001\u4eca\u65e5\u307e\u3067\u6301\u7d9a\u7684\u306b\u5ef6\u9577\u3055\u308c\u307e\u3057\u305f\u3002        \n")),(0,i.kt)("h2",{id:"language-detection"},"Language Detection"),(0,i.kt)("p",null,"This feature detects the language of the given text."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import os\n\nfrom google.cloud import translate_v2\n\nos.environ[\"PROJECT_ID\"] = \"text-analysis-323506\"\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.path.abspath(\"/home/jupyter/key.json\")\n\n# Create a Google Cloud translate api client object\ntranslate_client = translate_v2.Client()\n\ntext = 'D\xc9CRYPTAGESElectronique, bois, plastiques : les entreprises de la plan\xe8te sont confront\xe9es \xe0 un manque de mati\xe8res et de composants pour r\xe9pondre \xe0 la demande. Voici pourquoi.'\n\nresponse = translate_client.detect_language(text)\nresponse.get('language')\n\n# 'fr'\n\ntext = \"\u3053\u308c\u3089\u306b\u306f\u3001\u6c17\u5019\uff08\u4e0d\u4f5c\uff09\u3001\u653f\u6cbb\uff08\u6226\u4e89\u72b6\u6cc1\u3001\u5c01\u9396\uff09\u3001\u7d4c\u6e08\uff08\u4e0d\u5341\u5206\u306a\u751f\u7523\uff09\u3001\u3055\u3089\u306b\u306f\u7d14\u7c8b\u306b\u6295\u6a5f\u7684\u306a\u3082\u306e\u306a\u3069\u3001\u3055\u307e\u3056\u307e\u306a\u539f\u56e0\u304c\u8003\u3048\u3089\u308c\u307e\u3059\"\nresponse = translate_client.detect_language(text)\nresponse.get('language')\n\n# 'ja'\n\n")),(0,i.kt)("h2",{id:"video-intelligence-api"},"Video Intelligence API"),(0,i.kt)("p",null,"Video Intelligence API is used for common video processing problems.",(0,i.kt)("a",{parentName:"p",href:"https://console.cloud.google.com/marketplace/product/google/videointelligence.googleapis.com?project=flutter-217514"})),(0,i.kt)("h2",{id:"shot-change-detection"},"Shot Change Detection"),(0,i.kt)("p",null,"This feature detects changes in shots of the video."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import os\n\nfrom IPython.display import Video\nfrom google.cloud import videointelligence_v1 as videointelligence\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath("/home/jupyter/key.json")\n\n# Create Google Cloud Video Intelligence API client object\nvideo_client = videointelligence.VideoIntelligenceServiceClient()\n\nvideo_file_path = \'./data/audi_advertisement.mp4\'\n\nVideo(video_file_path)\n\n# Read video\nwith io.open(video_file_path, "rb") as f:\n    input_content = f.read()\n\n# create request\noperation = video_client.annotate_video(\n    request={\n        "features": [videointelligence.Feature.SHOT_CHANGE_DETECTION],\n        "input_content": input_content\n    }\n)\n\n# API endpoint provides start and end times for each shot in the video.\n\nresponse = operation.result(timeout=300)\n\n#Dumping response to a text file\n#Dumping responses to a text file as it\'s too big to be displayed\n\nwith open(\'./results/shot_change_detection.txt\', \'w\') as fstream:\n    fstream.write(str(response))\n\n')),(0,i.kt)("h2",{id:"speech-transcription"},"Speech Transcription"),(0,i.kt)("p",null,"This feature transcribes speech in the videos. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import os\n\nfrom IPython.display import Video\nfrom google.cloud import videointelligence_v1 as videointelligence\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath("/home/jupyter/key.json")\n\n# Create Google Cloud Video Intelligence API client object\nvideo_client = videointelligence.VideoIntelligenceServiceClient()\n\nvideo_file_path = \'./data/audi_advertisement.mp4\'\n\nVideo(video_file_path)\n\n# Read video\nwith io.open(video_file_path, "rb") as f:\n    input_content = f.read()\n\n# Configure the request\nconfig = videointelligence.SpeechTranscriptionConfig(\n    language_code="en-US", enable_automatic_punctuation=True\n)\n\ncontext = videointelligence.VideoContext(speech_transcription_config=config)\n\n# create request\noperation = video_client.annotate_video(\n        request={\n            "features": [videointelligence.Feature.SPEECH_TRANSCRIPTION],\n            "input_content": input_content,\n            "video_context": context,\n    }\n)\n\nresponse = operation.result(timeout=300)\n\n## Transcribed speech\nfor trans in response.annotation_results[0].speech_transcriptions:\n    print(trans.alternatives[0].transcript)\n\n# Ideas are beautiful. At first. They are inside you. Electrify you grow from our imagination.\n#  But ultimately, they want to be set, free out into the world, to disrupted, improve it to make it more meaningful more, beautiful, and even more incredible ideas for the future, just like that. The new fully electric Audi, RS, e-tron TT.\n\n#  Audi.\n')),(0,i.kt)("h2",{id:"label-detection-1"},"Label Detection"),(0,i.kt)("p",null,"This feature detects labels from videos."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import os\n\nfrom IPython.display import Video\nfrom google.cloud import videointelligence_v1 as videointelligence\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath("/home/jupyter/key.json")\n\n# Create Google Cloud Video Intelligence API client object\nvideo_client = videointelligence.VideoIntelligenceServiceClient()\n\nvideo_file_path = \'./data/audi_advertisement.mp4\'\n\nVideo(video_file_path)\n\n# Read video\nwith io.open(video_file_path, "rb") as f:\n    input_content = f.read()\n\n# create request\noperation = video_client.annotate_video(\n    request={\n        "features": [videointelligence.Feature.LABEL_DETECTION],\n        "input_content": input_content\n    }\n)\n\n# Results\nresponse = operation.result(timeout=300)\n\n# Major Labels returned by API\nfor annotations in response.annotation_results:\n    for shot in annotations.shot_label_annotations :\n        print(shot.entity.description)\n\n# head\n# formal wear\n# automotive design\n# mid size car\n# audi\n# motor vehicle\n# car\n# human\n# graphic design\n# emotion\n# portrait\n# suit\n# windshield\n# performance car\n# brand\n# hood\n# automotive exterior\n# luxury vehicle\n# mode of transport\n# personal luxury car\n# headlamp\n# coup\xe9\n# vehicle\n# executive car\n# logo\n# sky\n# daytime\n# automotive lighting\n# sports car\n# mouth\n# grille\n# audi\n# glass\n# nose\n# sedan\n# concept car\n# land vehicle\n# graphics\n# automotive window part\n# family car\n# black hair\n')),(0,i.kt)("h2",{id:"logo-detection-1"},"Logo Detection"),(0,i.kt)("p",null,"Detects all popular logos which appear in the video."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import os\n\nfrom IPython.display import Video\nfrom google.cloud import videointelligence_v1 as videointelligence\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath("/home/jupyter/key.json")\n\n# Create Google Cloud Video Intelligence API client object\nvideo_client = videointelligence.VideoIntelligenceServiceClient()\n\nvideo_file_path = \'./data/pepsi_advertisement.mp4\'\n\nVideo(video_file_path)\n\n# Read video\nwith io.open(video_file_path, "rb") as f:\n    input_content = f.read()\n\n# create request\noperation = video_client.annotate_video(\n    request={\n        "features": [videointelligence.Feature.LOGO_RECOGNITION],\n        "input_content": input_content\n    }\n)\n\nresponse = operation.result(timeout=300)\n\n# Results\nfor annotations in response.annotation_results:\n    for logo in annotations.logo_recognition_annotations:\n        print(f"Logo: {logo.entity.description}, Confidence: {logo.tracks[0].confidence}")\n\n# Logo: Pepsi, Confidence: 0.9548713564872742\n# Logo: Renault Samsung Motors, Confidence: 0.893693208694458\n# Logo: UEFA, Confidence: 0.9070633053779602\n')),(0,i.kt)("h2",{id:"people-detection"},"People Detection"),(0,i.kt)("p",null,"The feature detects people in the videos and provides coordinates to draw bounding boxes around detected people. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import os\n\nfrom IPython.display import Video\nfrom google.cloud import videointelligence_v1 as videointelligence\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath("/home/jupyter/key.json")\n\n# Create Google Cloud Video Intelligence API client object\nvideo_client = videointelligence.VideoIntelligenceServiceClient()\n\nvideo_file_path = \'./data/pepsi_advertisement.mp4\'\n\nVideo(video_file_path)\n\n# Read video\nwith io.open(video_file_path, "rb") as f:\n    input_content = f.read()\n\n# Configure the request\nconfig = videointelligence.PersonDetectionConfig(\n    include_bounding_boxes=True, include_attributes=True, include_pose_landmarks=True,\n)\n\ncontext = videointelligence.types.VideoContext(person_detection_config=config)\n\n# create request\noperation = video_client.annotate_video(\n        request={\n            "features": [videointelligence.Feature.PERSON_DETECTION],\n            "input_content": input_content,\n            "video_context": context,\n    }\n)\n\nresponse = operation.result(timeout=300)\n\n#Dumping response to a text file\n#Dumping response to a text as it\'s too big to be displayed on output box\n\nwith open(\'./results/people_detection.txt\', \'w\') as fstream:\n    fstream.write(str(response))\n')),(0,i.kt)("h2",{id:"face-detection-1"},"Face Detection"),(0,i.kt)("p",null,"This feature detects faces in the videos. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import os\n\nfrom IPython.display import Video\nfrom google.cloud import videointelligence_v1 as videointelligence\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath("/home/jupyter/key.json")\n\n# Create Google Cloud Video Intelligence API client object\nvideo_client = videointelligence.VideoIntelligenceServiceClient()\n\nvideo_file_path = \'./data/pepsi_advertisement.mp4\'\n\nVideo(video_file_path)\n\n# Read video\nwith io.open(video_file_path, "rb") as f:\n    input_content = f.read()\n\n# Configure the request\nconfig = videointelligence.FaceDetectionConfig(\n    include_bounding_boxes=True, include_attributes=True\n)\n\ncontext = videointelligence.VideoContext(face_detection_config=config)\n\n# create request\noperation = video_client.annotate_video(\n    request={\n        "features": [videointelligence.Feature.FACE_DETECTION],\n        "input_content": input_content,\n        "video_context": context,\n    }\n)\n\nresponse = operation.result(timeout=300)\n\n#Dumping response to a text file\n#Results are being dumped to a text file because the resoponse is extremely big\n\nwith open(\'./results/face_detection.txt\', \'w\') as fstream:\n    fstream.write(str(response))\n\n\n')),(0,i.kt)("h2",{id:"natural-language-api"},"Natural Language API"),(0,i.kt)("p",null,"Natural Language API is used to solve some of the common NLP problems.",(0,i.kt)("a",{parentName:"p",href:"https://console.cloud.google.com/marketplace/product/google/language.googleapis.com?project=flutter-217514"})),(0,i.kt)("h2",{id:"entity-analysis"},"Entity Analysis"),(0,i.kt)("p",null,"This feature analyzes the input text for known entities like proper names or common nouns. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import os\nimport io\n\nfrom google.cloud import language_v1\n\nos.environ["PROJECT_ID"] = "text-analysis-323506"\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath("/home/jupyter/key.json")\n\n# Create Google Cloud Video Intelligence API client object\nlanguage_client = language_v1.LanguageServiceClient()\n\ntext = "Google Cloud has wide range of machine learning APIs."\n\ndocument = {\'content\': text, \'type_\': language_v1.Document.Type.PLAIN_TEXT, \'language\': \'en\'}\n\nresponse  = language_client.analyze_entities(request= {\'document\': document})\n\nresponse\n\n# entities {\n#   name: "Google Cloud"\n#   type_: OTHER\n#   metadata {\n#     key: "mid"\n#     value: "/m/0105pbj4"\n#   }\n#   metadata {\n#     key: "wikipedia_url"\n#     value: "https://en.wikipedia.org/wiki/Google_Cloud_Platform"\n#   }\n#   salience: 0.5621581673622131\n#   mentions {\n#     text {\n#       content: "Google Cloud"\n#       begin_offset: -1\n#     }\n#     type_: PROPER\n#   }\n# }\n# entities {\n#   name: "range"\n#   type_: OTHER\n#   salience: 0.25460705161094666\n#   mentions {\n#     text {\n#       content: "range"\n#       begin_offset: -1\n#     }\n#     type_: COMMON\n#   }\n# }\n# entities {\n#   name: "machine learning APIs"\n#   type_: CONSUMER_GOOD\n#   salience: 0.1832347959280014\n#   mentions {\n#     text {\n#       content: "machine learning APIs"\n#       begin_offset: -1\n#     }\n#     type_: COMMON\n#   }\n# }\n# language: "en"\n')),(0,i.kt)("h2",{id:"sentiment-analysis"},"Sentiment Analysis"),(0,i.kt)("p",null,"This feature detects the sentiment of the input text. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import os\nimport io\n\nfrom google.cloud import language_v1\n\nos.environ[\"PROJECT_ID\"] = \"text-analysis-323506\"\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.path.abspath(\"/home/jupyter/key.json\")\n\n# Create Google Cloud Video Intelligence API client object\nlanguage_client = language_v1.LanguageServiceClient()\n\n# Positive Example\n# input text\ntext = \"This movie is so good. Loved every second of it !!\"\n\ndocument = {'content': text, 'type_': language_v1.Document.Type.PLAIN_TEXT, 'language': 'en'}\n\nresponse  = language_client.analyze_sentiment(request= {'document': document})\n\n# Document Sentiment\nresponse.document_sentiment\n\n# magnitude: 1.899999976158142\n# score: 0.8999999761581421\n# The score shows that the input text has a very positive sentiment. (Towards +1)\n\n# Sentiments for each sentence in the input text\nfor sentence in response.sentences:\n    print(\"Text: \")\n    print(sentence.text.content)\n    print(\"Sentiment: \")\n    print(sentence.sentiment)\n\n# Text: \n# This movie is so good.\n# Sentiment: \n# magnitude: 0.8999999761581421\n# score: 0.8999999761581421\n\n# Text: \n# Loved every second of it !!\n# Sentiment: \n# magnitude: 0.8999999761581421\n# score: 0.8999999761581421\n\n# Negative Example\n\n# input text\ntext = \"One of the worst movies ever!!. Please don't waste your time watching it..\"\n\ndocument = {'content': text, 'type_': language_v1.Document.Type.PLAIN_TEXT, 'language': 'en'}\n\nresponse  = language_client.analyze_sentiment(request= {'document': document})\n\n# Document Sentiment\nresponse.document_sentiment\n\n# magnitude: 1.5\n# score: -0.699999988079071\n# The score shows that the input text has a very negative sentiment. (Towards -1)\n\n# Sentiments for each sentence in the input text\nfor sentence in response.sentences:\n    print(\"Text: \")\n    print(sentence.text.content)\n    print(\"Sentiment: \")\n    print(sentence.sentiment)\n\n# Text: \n# One of the worst movies ever!!.\n# Sentiment: \n# magnitude: 0.800000011920929\n# score: -0.800000011920929\n\n# Text: \n# Please don't waste your time watching it..\n# Sentiment: \n# magnitude: 0.699999988079071\n# score: -0.699999988079071\n")),(0,i.kt)("h2",{id:"syntax-analysis"},"Syntax Analysis"),(0,i.kt)("p",null,"This feature analyses the input text and returns syntactic details."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import os\nimport io\n\nfrom google.cloud import language_v1\n\nos.environ["PROJECT_ID"] = "text-analysis-323506"\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath("/home/jupyter/key.json")\n\n# Create Google Cloud Video Intelligence API client object\nlanguage_client = language_v1.LanguageServiceClient()\n\ntext = "Google Cloud has wide range of machine learning APIs."\n\ndocument = {\'content\': text, \'type_\': language_v1.Document.Type.PLAIN_TEXT, \'language\': \'en\'}\n\nresponse  = language_client.analyze_syntax(request= {\'document\': document})\n\nresponse\n\n# sentences {\n#   text {\n#     content: "Google Cloud has wide range of machine learning APIs."\n#     begin_offset: -1\n#   }\n# }\n# tokens {\n#   text {\n#     content: "Google"\n#     begin_offset: -1\n#   }\n#   part_of_speech {\n#     tag: NOUN\n#     number: SINGULAR\n#     proper: PROPER\n#   }\n#   dependency_edge {\n#     head_token_index: 1\n#     label: NN\n#   }\n#   lemma: "Google"\n# }\n# tokens {\n#   text {\n#     content: "Cloud"\n#     begin_offset: -1\n#   }\n#   part_of_speech {\n#     tag: NOUN\n#     number: SINGULAR\n#     proper: PROPER\n#   }\n#   dependency_edge {\n#     head_token_index: 2\n#     label: NSUBJ\n#   }\n#   lemma: "Cloud"\n# }\n# tokens {\n#   text {\n#     content: "has"\n#     begin_offset: -1\n#   }\n#   part_of_speech {\n#     tag: VERB\n#     mood: INDICATIVE\n#     number: SINGULAR\n#     person: THIRD\n#     tense: PRESENT\n#   }\n#   dependency_edge {\n#     head_token_index: 2\n#     label: ROOT\n#   }\n#   lemma: "have"\n# }\n# tokens {\n#   text {\n#     content: "wide"\n#     begin_offset: -1\n#   }\n#   part_of_speech {\n#     tag: ADJ\n#   }\n#   dependency_edge {\n#     head_token_index: 4\n#     label: AMOD\n#   }\n#   lemma: "wide"\n# }\n# tokens {\n#   text {\n#     content: "range"\n#     begin_offset: -1\n#   }\n#   part_of_speech {\n#     tag: NOUN\n#     number: SINGULAR\n#   }\n#   dependency_edge {\n#     head_token_index: 2\n#     label: DOBJ\n#   }\n#   lemma: "range"\n# }\n# tokens {\n#   text {\n#     content: "of"\n#     begin_offset: -1\n#   }\n#   part_of_speech {\n#     tag: ADP\n#   }\n#   dependency_edge {\n#     head_token_index: 4\n#     label: PREP\n#   }\n#   lemma: "of"\n# }\n# tokens {\n#   text {\n#     content: "machine"\n#     begin_offset: -1\n#   }\n#   part_of_speech {\n#     tag: NOUN\n#     number: SINGULAR\n#   }\n#   dependency_edge {\n#     head_token_index: 7\n#     label: NN\n#   }\n#   lemma: "machine"\n# }\n# tokens {\n#   text {\n#     content: "learning"\n#     begin_offset: -1\n#   }\n#   part_of_speech {\n#     tag: NOUN\n#     number: SINGULAR\n#   }\n#   dependency_edge {\n#     head_token_index: 8\n#     label: NN\n#   }\n#   lemma: "learning"\n# }\n# tokens {\n#   text {\n#     content: "APIs"\n#     begin_offset: -1\n#   }\n#   part_of_speech {\n#     tag: NOUN\n#     number: SINGULAR\n#   }\n#   dependency_edge {\n#     head_token_index: 5\n#     label: POBJ\n#   }\n#   lemma: "api"\n# }\n# tokens {\n#   text {\n#     content: "."\n#     begin_offset: -1\n#   }\n#   part_of_speech {\n#     tag: PUNCT\n#   }\n#   dependency_edge {\n#     head_token_index: 2\n#     label: P\n#   }\n#   lemma: "."\n# }\n# language: "en"\n')),(0,i.kt)("h2",{id:"text-classification"},"Text Classification"),(0,i.kt)("p",null,"This feature classifies the input text into different known categories. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import os\nimport io\n\nfrom google.cloud import language_v1\n\nos.environ[\"PROJECT_ID\"] = \"text-analysis-323506\"\n\n# Setting path to key.json as GOOGLE_APPLICATION_CREDENTIALS environment variable\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.path.abspath(\"/home/jupyter/key.json\")\n\n# Create Google Cloud Video Intelligence API client object\nlanguage_client = language_v1.LanguageServiceClient()\n\ntext = \"China has started shutting down schools, cancelling hundreds of \\\n        flights and ramping up mass testings following a new outbreak of \\\n        Covid-19\"\n\ndocument = {'content': text, 'type_': language_v1.Document.Type.PLAIN_TEXT, 'language': 'en'}\n\nresponse  = language_client.classify_text(request= {'document': document})\n\nresponse\n\n# categories {\n#   name: \"/Travel/Air Travel\"\n#   confidence: 0.8999999761581421\n# }\n# Example 2:\n\ntext = \"Mangoes are one of the tastiest fruits out there. I mean, \\\n        who doesn't like mangoes?. There are a lot of different types \\\n        of mangoes. Each of them look and taste slightly different.\"\n\ndocument = {'content': text, 'type_': language_v1.Document.Type.PLAIN_TEXT, 'language': 'en'}\n\nresponse  = language_client.classify_text(request= {'document': document})\n\nresponse\n\n# categories {\n#   name: \"/Food & Drink/Food\"\n#   confidence: 0.9800000190734863\n# }\n")))}d.isMDXComponent=!0}}]);